{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First of all let's have look at \"official\" example : Matrix dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU timing:\n",
      "CPU times: user 297 µs, sys: 140 µs, total: 437 µs\n",
      "Wall time: 299 µs\n",
      "\n",
      "CPU timing:\n",
      "CPU times: user 13.6 ms, sys: 2.2 ms, total: 15.8 ms\n",
      "Wall time: 5.99 ms\n",
      "\n",
      "Check GPU result with CPU result if they are the same :\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import reikna.cluda as cluda\n",
    "from reikna.linalg import MatrixMul\n",
    "\n",
    "api = cluda.ocl_api()#.cuda_api() switch opencl and cuda\n",
    "# thr = api.Thread.create()\n",
    "\n",
    "shape1 = (3000, 784)\n",
    "shape2 = (784, 100)\n",
    "\n",
    "a = numpy.random.randn(*shape1).astype(numpy.float32)\n",
    "b = numpy.random.randn(*shape2).astype(numpy.float32)\n",
    "\n",
    "# Matrix dot at GPU must have a constant shape and defined array to GPU device\n",
    "a_dev = thr.to_device(a)\n",
    "b_dev = thr.to_device(b)\n",
    "res_dev = thr.array((shape1[0], shape2[1]), dtype=numpy.float32)\n",
    "\n",
    "# fun definition, a_dev dot b_dev = res_dev\n",
    "dot = MatrixMul(a_dev, b_dev, out_arr=res_dev)\n",
    "\n",
    "# fun coompile, because of constant shape, different shape dot need different fun definition , *in fact it can be not if write at your own code.\n",
    "\n",
    "fun_3000_784Dot784_100 = dot.compile(thr)\n",
    "\n",
    "print(\"GPU timing:\")\n",
    "%time fun_3000_784Dot784_100(res_dev, a_dev, b_dev)\n",
    "print()\n",
    "print(\"CPU timing:\")\n",
    "%time res_reference = numpy.dot(a, b)\n",
    "\n",
    "print()\n",
    "print(\"Check GPU result with CPU result if they are the same :\")\n",
    "print(norm(res_dev.get() - res_reference) / norm(res_reference) < 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above showed that GPU is ten times faster at least.\n",
    "## 2. Writing own code : Matrix element wise operation\n",
    "Q1.How GPU working?\n",
    "\n",
    "Don't think things of difficulty. It just about parallel working. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(0, 1)\n",
      "(1, 0)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        print((i,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above printed shapes in order. But parallel working will print them disorder. \n",
    "\n",
    "This session is about Matrix element wise operation which does not care of the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU timing:\n",
      "CPU times: user 305 µs, sys: 193 µs, total: 498 µs\n",
      "Wall time: 271 µs\n",
      "\n",
      "CPU timing:\n",
      "CPU times: user 19.9 ms, sys: 7.68 ms, total: 27.6 ms\n",
      "Wall time: 20.7 ms\n",
      "\n",
      "Check GPU result with CPU result if they are the same :\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import reikna.cluda as cluda\n",
    "\n",
    "api = cluda.ocl_api()#.cuda_api() switch opencl and cuda\n",
    "thr = api.Thread.create()\n",
    "\n",
    "shape = (3000, 3000)\n",
    "\n",
    "a = numpy.random.randn(*shape).astype(numpy.float32)\n",
    "b = numpy.random.randn(*shape).astype(numpy.float32)\n",
    "\n",
    "a_dev = thr.to_device(a)\n",
    "b_dev = thr.to_device(b)\n",
    "res_dev = thr.array( (shape), dtype=numpy.float32)\n",
    "\n",
    "# write code string, c code, below code is called \"kernel\" which will run parallelly at GPU\n",
    "program = thr.compile(\"\"\"\n",
    "KERNEL void gpu_prod(\n",
    "    GLOBAL_MEM float *a_dev,\n",
    "    GLOBAL_MEM float *b_dev,\n",
    "    GLOBAL_MEM float *res_dev\n",
    "    )\n",
    "{\n",
    "    //This id0 likes first layer of for loop's  i value from built-in fun get_global_id(int i)\n",
    "    \n",
    "    const SIZE_T id0 = get_global_id(0);\n",
    "    \n",
    "\n",
    "    //This id1 likes second layer of for loop's  j value\n",
    "    \n",
    "    const SIZE_T id1 = get_global_id(1);\n",
    "    \n",
    "\n",
    "    //This is most important line, calculate the exact index of the value at [i,j]\n",
    "    //At GPU, all arrays are treated as one dimension array.\n",
    "    //So a_dev has shape(3000, 3000) a_dev[i,j] == a_dev[i*3000 + j],\n",
    "    //For example: [0,1]==[0*3000+1]=[1], [1,1]==[1*3000 + 1]=[3001]\n",
    "    //get_global_size(1) : get shape[1]\n",
    "    \n",
    "    int IDX = id0*get_global_size(1)+id1;\n",
    "\n",
    "\n",
    "    res_dev[IDX] = a_dev[IDX] * b_dev[IDX] ; // calculate product value, it can be +,/,<... etc.\n",
    "}\n",
    "\"\"\")\n",
    "#have attention to the function name if it is same as above\n",
    "GPU_prod = program.gpu_prod\n",
    "\n",
    "# parameter of local_size will never used, but need to keep same list len as global_size and set all value to 1\n",
    "# global_size will set get_global_size() 's value directly\n",
    "print(\"GPU timing:\")\n",
    "%time GPU_prod(a_dev, b_dev, res_dev, local_size=(1,1), global_size=res_dev.shape)\n",
    "print()\n",
    "print(\"CPU timing:\")\n",
    "%time res_reference = a*b\n",
    "\n",
    "print()\n",
    "print(\"Check GPU result with CPU result if they are the same :\")\n",
    "print(norm(res_dev.get() - res_reference) / norm(res_reference) < 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
